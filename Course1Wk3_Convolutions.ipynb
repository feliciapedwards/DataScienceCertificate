{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course1Wk3_Convolutions",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feliciapedwards/TensorFlow-with-Laurence-Moroney/blob/master/Course1Wk3_Convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeM_Do6sJA0G",
        "colab_type": "text"
      },
      "source": [
        "Improving our model using Convolutions.\n",
        "For review, lets take a look at our model from last week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdL9_2zpJMFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "e2684b0c-4d39-47d0-98e2-b7f85825fd2d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4926 - acc: 0.8274\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3728 - acc: 0.8643\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3325 - acc: 0.8792\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3077 - acc: 0.8884\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.2918 - acc: 0.8928\n",
            "10000/10000 [==============================] - 0s 33us/sample - loss: 0.3435 - acc: 0.8751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV4-M6VRKZdC",
        "colab_type": "text"
      },
      "source": [
        "Our training data has 89% accuracy while our test data has 87% accuracy. How do we improve this? One way is through convolutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NE2WQKsJZDQ",
        "colab_type": "text"
      },
      "source": [
        "Convolutions try to pinpoint specific details of the image that help distinguish them. We add some layers to create convolution before the dense layers. This refines the information that is passed down to the dense layers, hopefully making it more concentrated and focused."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3kMOennKCxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "050777d0-c0c2-4644-e9aa-a73be2a76a2a"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.4476 - acc: 0.8386\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.3007 - acc: 0.8922\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.2559 - acc: 0.9053\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 82s 1ms/sample - loss: 0.2216 - acc: 0.9179\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 81s 1ms/sample - loss: 0.1958 - acc: 0.9266\n",
            "10000/10000 [==============================] - 4s 403us/sample - loss: 0.2442 - acc: 0.9113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AlwQrR1LYHc",
        "colab_type": "text"
      },
      "source": [
        "Here we had to shape both our training and test data to (60000, 28, 28, 1) and (10000, 28, 28, 1). This is critical, otherwise we'll get an error since the Convolutions wont be able to recognize the shape. \n",
        "\n",
        "The first layer \"tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1))\" in our model instructs keras to generate 64 filters. These filters are 3 by 3, their activation is relu, which means we only keep our 0 or larger values, and  the input shape 28 by 28 is maintained. Since our images are gray scale, we use \"1\" for a single byte for color depth.\n",
        "\n",
        "The second layer \"tf.keras.layers.MaxPooling2D(2, 2),\" creates a pooling layer. It's called a max-pooling because we're taking the maximum value. We're specifying a two-by-two pool, so for every four pixels, the largest value will be the values that is passed on. This reduces the image size by a quarter.\n",
        "\n",
        "The second and third layer are created so that the network can learn an additional set of convolutions, and we pool again to reduce the size again. So when all is said and done, the images have been quartered twice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnT2V0jYjUXi",
        "colab_type": "text"
      },
      "source": [
        "The training time increased substantially from 4 seconds to roughly 80 second. However, the training data accuracy is 93% and the test data accuracy is 91%. A huge improvement!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b45wJD75jLJ",
        "colab_type": "text"
      },
      "source": [
        "Lets change the number of convolutons from 64 to 32 and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QK2-XX15fmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "186354df-99c5-4e37-aa1c-c3b40a6625b6"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 113,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 45s 742us/sample - loss: 0.4795 - acc: 0.8248\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 43s 725us/sample - loss: 0.3230 - acc: 0.8818\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 44s 729us/sample - loss: 0.2740 - acc: 0.8985\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 43s 722us/sample - loss: 0.2432 - acc: 0.9096\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 44s 733us/sample - loss: 0.2192 - acc: 0.9184\n",
            "10000/10000 [==============================] - 2s 245us/sample - loss: 0.2858 - acc: 0.8941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MguDIy91DglO",
        "colab_type": "text"
      },
      "source": [
        "Looks like our model trained almost twice as fast, however the accuracy of our training data went down to 92% and the accuracy of our test data went down as well to 90%. This worsened the performance of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3HqXxLxEsW9",
        "colab_type": "text"
      },
      "source": [
        "We are going to see what happens if we remove the second convolution, how this impacts our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCx_ktsWEnu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "5c917958-a8bb-42ea-8cb2-7ec19f4cf64b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               1384576   \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,386,506\n",
            "Trainable params: 1,386,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 57s 948us/sample - loss: 0.3778 - acc: 0.8665\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 55s 922us/sample - loss: 0.2569 - acc: 0.9062\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 55s 922us/sample - loss: 0.2125 - acc: 0.9228\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 55s 924us/sample - loss: 0.1768 - acc: 0.9343\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 56s 926us/sample - loss: 0.1480 - acc: 0.9454\n",
            "10000/10000 [==============================] - 3s 252us/sample - loss: 0.2534 - acc: 0.9118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2dvI5kAGAsE",
        "colab_type": "text"
      },
      "source": [
        "Interesting! So our training accuracy is roughly 95%, and our test data accuracy is roughly 91%. If we look at the training numbers, it is slightly higher than when we used two convolutions. Our training time was reduced by roughly 30 seconds for each epoch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU0DA9OcH84k",
        "colab_type": "text"
      },
      "source": [
        "What happens when we add a convolution?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaZ_eDQoH3at",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "f299f92d-edd8-4f89-d855-34b4aef0b7f9"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 84,106\n",
            "Trainable params: 84,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.5781 - acc: 0.7895\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.3833 - acc: 0.8577\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.3295 - acc: 0.8781\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.2956 - acc: 0.8920\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.2719 - acc: 0.8997\n",
            "10000/10000 [==============================] - 4s 405us/sample - loss: 0.3562 - acc: 0.8733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiCyKAlTPnty",
        "colab_type": "text"
      },
      "source": [
        "Looks like our training time increased by roughly 30 seconds for each epoch, and the training accuracy was reduced to 90% and test data at 87%. Adding more convolutions in this example didn't improve our model."
      ]
    }
  ]
}